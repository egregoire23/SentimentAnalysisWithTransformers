# Sentiment Analysis with Transformers
Transformers are a leading architecture in modern Deep Learning. Utilizing the Pytorch framework, I collaborated with a teammate to implement a Transformer model from scratch. We applied a variety of optimization techniques and performed hyperparameter tuning to classify reviews as positive or negative with high accuracy.

Skills:

Natural Language Processing, PyTorch, Hyperparameter Tuning, Classification, Tokenization

Dataset:
- Amazon Reviews dataset ([link](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M?resourcekey=0-TLwzfR2O-D2aPitmn5o9VQ))

Methodology:
- Preprocessed the dataset and conducted exploratory data analysis
- Build a Transformer model from scratch
- Trained the model and tuned hyperparameters
- Evaluated performance using standard classification metrics

Tools & Technologies:
- Programming Language: Python
- Libraries: torch, pandas, seaborn, matplotlib, sklearn
- Software: Jupyter Notebook

Results & Evaluation:

After exploring a multitude of optimization techniques, our model achieved a test accuracy of 82.9%, a loss of 0.38, and an F1 score of 0.82. These results indicate strong performance on the classification task.

Challenges & Learning:

I deepened my understanding of Transformer components in PyTorch, including multi-head attention, encoder-decoder architecture, and embeddings. I also implemented a custom positional encoding class and gained valuable experience in tokenizing and preparing text data for model training.

Contribution:
- Team Members: Erin Gregoire & Dawson Damuth
- My Role: Led the development of the Transformer model, conducted training, hyperparameter tuning, and model evaluation
