# Sentiment Analysis with Transformers
Transformers are a hot topic in the field of Deep Learning. Utilizing the Pytorch framework, I worked with a teammate to implement a transformer model from scratch. I explored various optimization techniques and hyperparameter tuning to achieve optimal performance for the classification of positive or negative reviews.

Skills:

PyTorch, Hyperparameter Tuning, Classification, Tokenizing Text Data

Dataset:
- Amazon Reviews dataset ([link](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M?resourcekey=0-TLwzfR2O-D2aPitmn5o9VQ))

Methodology:
- Preprocess the dataset and perform exploratory data analysis
- Build a Transformer model from scratch
- Train and tune hyperparameters
- Evaluate and provide results

Tools & Technologies:
- Programming Language: Python
- Libraries: torch, pandas, seaborn, matplotlib, sklearn
- Software: Jupyter Notebook

Results & Evaluation:

After exploring a multitude of optimization techniques, the final test accuracy of the transformer was 82.9% with a loss score of .38. The F1 score achieved 0.82 and overall performance was stellar!

Challenges & Learning:

I learned how to utilize the various blocks that PyTorch has for Transformers, including multi-attention heads, encoding vs decoding, and embedding. I also learned how to implement a custom positional encoding class. Lastly, I learned the importance of tokening text datasets.

Contribution:
- Team Members: Erin Gregoire & Dawson Damuth
- My Role: Developed the Transformer model, and conducted training, hyperparameter tuning, and evaluation of the model
